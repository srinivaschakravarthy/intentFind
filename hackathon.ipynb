{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    lines=open('data/data.txt', encoding='utf-8', errors='ignore')\n",
    "    lines=lines.read()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines(lines):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lines=lines.split('\\n')\n",
    "    id2line = {}\n",
    "    line_id = 0;\n",
    "    for line in lines:\n",
    "        tok_line = word_tokenize(line)\n",
    "        stp_line = [w for w in tok_line if not w in stop_words]\n",
    "        id2line[line_id] = stp_line\n",
    "        line_id+=1\n",
    "    return id2line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS AND LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_lemmatize(id2lines):\n",
    "    posLem = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in id2lines:\n",
    "        line = id2lines[i]\n",
    "        pos_line = nltk.pos_tag(line)\n",
    "        v_n_line = []\n",
    "        for pair in pos_line:\n",
    "            pos = find_tag(pair[1])\n",
    "            if(pos == 'n' or pos == 'v'):\n",
    "                lemm = lemmatizer.lemmatize(pair[0], pos)\n",
    "                pair_lem = (lemm, pos)\n",
    "                v_n_line.append(pair_lem)\n",
    "        posLem[i] = v_n_line\n",
    "    print (posLem)\n",
    "    return posLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag(value):\n",
    "    if(value.startswith('A')):\n",
    "        return 'a'\n",
    "    elif(value.startswith('V')):\n",
    "        return 'v'\n",
    "    elif(value.startswith('R')):\n",
    "        return 'r'\n",
    "    elif(value.startswith('N')):\n",
    "        return 'n'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(poslines):\n",
    "    score2word = {} #{word : [{pos: score},()],[] }\n",
    "    for i in poslines:\n",
    "        line = poslines[i]\n",
    "               \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2lines = tokenize_lines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['Can', 'make', 'quick', '?', 'Roxanne', 'Korrine', 'Andrew', 'Barrett', 'incredibly', 'horrendous', 'public', 'break-', 'quad', '.', 'Again', '.'], 1: ['Well', ',', '...'], 2: ['Me', '.', 'This', 'endless', '...', 'blonde', 'babble', '.', 'I', \"'m\", 'like', ',', 'boring', '.'], 3: ['I', 'figured', \"'d\", 'get', 'good', 'stuff', 'eventually', '.'], 4: ['The', '``', 'real', \"''\", '.'], 5: ['I', \"'m\", 'kidding', '.', 'You', 'know', 'sometimes', 'become', '``', 'persona', \"''\", '?', 'And', \"n't\", 'know', 'quit', '?'], 6: ['She', 'okay', '?'], 7: ['Where', 'go', '?', 'He', '.'], 8: ['It', \"'s\"], 9: ['Neat', '...'], 10: ['I', 'potential', 'smack', 'crap', \"n't\", 'get', 'way', '.'], 11: ['I', \"n't\", 'get', '.', 'You', 'act', 'like', \"'re\", 'good', ',', 'go', 'totally', 'apeshit', 'get', '.'], 12: ['I', 'care', '.', 'But', 'I', \"'m\", 'firm', 'believer', 'something', 'reasons', ',', 'someone', 'else', \"'\", '.'], 13: ['Patrick', '--', 'that-', '.'], 14: ['Now', \"n't\", 'get', 'upset', '.', 'Daddy', ',', \"'s\", 'boy', '...', 'I', 'think', 'might', 'ask', '...'], 15: ['Daddy', ',', 'I', '--'], 16: ['Daddy', ',', 'I', 'want', 'discuss', 'prom', '.', 'It', \"'s\", 'tomorrow', 'night', '--'], 17: ['He', \"'s\", '``', 'hot', 'rod', \"''\", '.', 'Whatever', '.'], 18: ['Fan', 'fan', '.', 'You', 'see', 'couple', 'minors', 'come', '?'], 19: ['So', '--', 'Dakota', '?']}\n"
     ]
    }
   ],
   "source": [
    "print(id2lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('make', 'v'), ('Roxanne', 'n'), ('Korrine', 'n'), ('Andrew', 'n'), ('Barrett', 'n'), ('break-', 'n'), ('quad', 'n'), ('Again', 'n')], 1: [], 2: [('Me', 'n'), ('endless', 'n'), ('blonde', 'n'), (\"'m\", 'v'), ('boring', 'n')], 3: [('figure', 'v'), ('get', 'v'), ('stuff', 'n')], 4: [], 5: [(\"'m\", 'v'), ('kid', 'v'), ('know', 'v'), ('become', 'v'), ('know', 'v'), ('quit', 'v')], 6: [], 7: [('go', 'v')], 8: [(\"'s\", 'v')], 9: [('Neat', 'n')], 10: [('potential', 'v'), ('smack', 'n'), ('crap', 'n'), ('get', 'v'), ('way', 'n')], 11: [('get', 'v'), ('act', 'v'), (\"'re\", 'v'), ('go', 'v'), ('get', 'n')], 12: [('care', 'v'), (\"'m\", 'v'), ('something', 'n'), ('reason', 'n'), ('someone', 'n')], 13: [('Patrick', 'n')], 14: [('get', 'v'), ('Daddy', 'n'), ('boy', 'n'), ('think', 'v'), ('ask', 'v')], 15: [('Daddy', 'n')], 16: [('Daddy', 'n'), ('want', 'v'), ('prom', 'n'), (\"'s\", 'v'), ('tomorrow', 'n'), ('night', 'n')], 17: [(\"'s\", 'v'), ('rod', 'n')], 18: [('Fan', 'n'), ('fan', 'n'), ('see', 'v'), ('minor', 'n'), ('come', 'v')], 19: [('Dakota', 'n')]}\n"
     ]
    }
   ],
   "source": [
    "poslines = pos_lemmatize(id2lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poslines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(poslines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "runnable\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "lemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
