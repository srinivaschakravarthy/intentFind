{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    lines=open('data/data.txt', encoding='utf-8', errors='ignore')\n",
    "    lines=lines.read()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines(lines):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lines=lines.split('\\n')\n",
    "    id2line = {}\n",
    "    line_id = 0;\n",
    "    for line in lines:\n",
    "        tok_line = word_tokenize(line)\n",
    "        stp_line = [w for w in tok_line if not w in stop_words]\n",
    "        id2line[line_id] = stp_line\n",
    "        line_id+=1\n",
    "    return id2line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS AND LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_lemmatize(id2lines):\n",
    "    posLem = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in id2lines:\n",
    "        line = id2lines[i]\n",
    "        pos_line = nltk.pos_tag(line)\n",
    "        for pair in pos_line:\n",
    "            pos = find_tag(pair[1])\n",
    "            lemm = lemmatizer.lemmatize(pair[0], pos)\n",
    "        posLem[i] = pos_line\n",
    "        print (pos_line)\n",
    "    return posLem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tag(value):\n",
    "    if(value.startswith('A')):\n",
    "        return 'a'\n",
    "    elif(value.startswith('V')):\n",
    "        return 'v'\n",
    "    elif(value.startswith('R')):\n",
    "        return 'r'\n",
    "    elif(value.startswith('N')):\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(lines):\n",
    "    tok_lines = word_tokenize(lines)\n",
    "    #print(len(set(tok_lines))/len(tok_lines))\n",
    "    unique_words = sorted(set(tok_lines))\n",
    "    pos_words = nltk.pos_tag(tok_lines)\n",
    "    score2word = {} #{word : [(pos, score),()],[] }\n",
    "    for word in unique_words:\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2lines = tokenize_lines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['Can', 'make', 'quick', '?', 'Roxanne', 'Korrine', 'Andrew', 'Barrett', 'incredibly', 'horrendous', 'public', 'break-', 'quad', '.', 'Again', '.'], 1: ['Well', ',', '...'], 2: ['Me', '.', 'This', 'endless', '...', 'blonde', 'babble', '.', 'I', \"'m\", 'like', ',', 'boring', '.'], 3: ['I', 'figured', \"'d\", 'get', 'good', 'stuff', 'eventually', '.'], 4: ['The', '``', 'real', \"''\", '.'], 5: ['I', \"'m\", 'kidding', '.', 'You', 'know', 'sometimes', 'become', '``', 'persona', \"''\", '?', 'And', \"n't\", 'know', 'quit', '?'], 6: ['She', 'okay', '?'], 7: ['Where', 'go', '?', 'He', '.'], 8: ['It', \"'s\"], 9: ['Neat', '...'], 10: ['I', 'potential', 'smack', 'crap', \"n't\", 'get', 'way', '.'], 11: ['I', \"n't\", 'get', '.', 'You', 'act', 'like', \"'re\", 'good', ',', 'go', 'totally', 'apeshit', 'get', '.'], 12: ['I', 'care', '.', 'But', 'I', \"'m\", 'firm', 'believer', 'something', 'reasons', ',', 'someone', 'else', \"'\", '.'], 13: ['Patrick', '--', 'that-', '.'], 14: ['Now', \"n't\", 'get', 'upset', '.', 'Daddy', ',', \"'s\", 'boy', '...', 'I', 'think', 'might', 'ask', '...'], 15: ['Daddy', ',', 'I', '--'], 16: ['Daddy', ',', 'I', 'want', 'discuss', 'prom', '.', 'It', \"'s\", 'tomorrow', 'night', '--'], 17: ['He', \"'s\", '``', 'hot', 'rod', \"''\", '.', 'Whatever', '.'], 18: ['Fan', 'fan', '.', 'You', 'see', 'couple', 'minors', 'come', '?'], 19: ['So', '--', 'Dakota', '?']}\n"
     ]
    }
   ],
   "source": [
    "print(id2lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Can', 'MD'), ('make', 'VB'), ('quick', 'JJ'), ('?', '.'), ('Roxanne', 'NNP'), ('Korrine', 'NNP'), ('Andrew', 'NNP'), ('Barrett', 'NNP'), ('incredibly', 'RB'), ('horrendous', 'JJ'), ('public', 'JJ'), ('break-', 'NN'), ('quad', 'NN'), ('.', '.'), ('Again', 'NNP'), ('.', '.')]\n",
      "[('Well', 'RB'), (',', ','), ('...', ':')]\n",
      "[('Me', 'NNP'), ('.', '.'), ('This', 'DT'), ('endless', 'NN'), ('...', ':'), ('blonde', 'NN'), ('babble', 'JJ'), ('.', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('like', 'IN'), (',', ','), ('boring', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('figured', 'VBD'), (\"'d\", 'MD'), ('get', 'VB'), ('good', 'JJ'), ('stuff', 'NN'), ('eventually', 'RB'), ('.', '.')]\n",
      "[('The', 'DT'), ('``', '``'), ('real', 'JJ'), (\"''\", \"''\"), ('.', '.')]\n",
      "[('I', 'PRP'), (\"'m\", 'VBP'), ('kidding', 'VBG'), ('.', '.'), ('You', 'PRP'), ('know', 'VBP'), ('sometimes', 'RB'), ('become', 'VBN'), ('``', '``'), ('persona', 'JJ'), (\"''\", \"''\"), ('?', '.'), ('And', 'CC'), (\"n't\", 'RB'), ('know', 'VB'), ('quit', 'VB'), ('?', '.')]\n",
      "[('She', 'PRP'), ('okay', 'PRP'), ('?', '.')]\n",
      "[('Where', 'WRB'), ('go', 'VB'), ('?', '.'), ('He', 'PRP'), ('.', '.')]\n",
      "[('It', 'PRP'), (\"'s\", 'VBZ')]\n",
      "[('Neat', 'NN'), ('...', ':')]\n",
      "[('I', 'PRP'), ('potential', 'VBP'), ('smack', 'NN'), ('crap', 'NN'), (\"n't\", 'RB'), ('get', 'VB'), ('way', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), (\"n't\", 'RB'), ('get', 'VB'), ('.', '.'), ('You', 'PRP'), ('act', 'VBP'), ('like', 'IN'), (\"'re\", 'VBP'), ('good', 'JJ'), (',', ','), ('go', 'VB'), ('totally', 'RB'), ('apeshit', 'JJ'), ('get', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('care', 'VBP'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), (\"'m\", 'VBP'), ('firm', 'JJ'), ('believer', 'IN'), ('something', 'NN'), ('reasons', 'NNS'), (',', ','), ('someone', 'NN'), ('else', 'RB'), (\"'\", 'POS'), ('.', '.')]\n",
      "[('Patrick', 'NNP'), ('--', ':'), ('that-', 'JJ'), ('.', '.')]\n",
      "[('Now', 'RB'), (\"n't\", 'RB'), ('get', 'VB'), ('upset', 'JJ'), ('.', '.'), ('Daddy', 'NNP'), (',', ','), (\"'s\", 'POS'), ('boy', 'NN'), ('...', ':'), ('I', 'PRP'), ('think', 'VBP'), ('might', 'MD'), ('ask', 'VB'), ('...', ':')]\n",
      "[('Daddy', 'NNP'), (',', ','), ('I', 'PRP'), ('--', ':')]\n",
      "[('Daddy', 'NNP'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('discuss', 'JJ'), ('prom', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('tomorrow', 'NN'), ('night', 'NN'), ('--', ':')]\n",
      "[('He', 'PRP'), (\"'s\", 'VBZ'), ('``', '``'), ('hot', 'JJ'), ('rod', 'NN'), (\"''\", \"''\"), ('.', '.'), ('Whatever', 'WDT'), ('.', '.')]\n",
      "[('Fan', 'NNP'), ('fan', 'NN'), ('.', '.'), ('You', 'PRP'), ('see', 'VBP'), ('couple', 'JJ'), ('minors', 'NNS'), ('come', 'VB'), ('?', '.')]\n",
      "[('So', 'RB'), ('--', ':'), ('Dakota', 'NNP'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "poslines = pos_lemmatize(id2lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poslines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "runnable\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "lemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
